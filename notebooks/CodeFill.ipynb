{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "CodeFill.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Install the correct dependencies on HuggingFace transformer and ternsorflow"
   ],
   "metadata": {
    "id": "xIT_uHUdThub"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f3KWTckbTUs2",
    "outputId": "2012bedd-8fb1-4909-d4bc-1f0ce0ad416e",
    "ExecuteTime": {
     "end_time": "2024-12-19T10:28:28.569814Z",
     "start_time": "2024-12-19T10:28:28.563824Z"
    }
   },
   "outputs": [],
   "source": [
    "# # We won't need TensorFlow here\n",
    "# !pip uninstall -y tensorflow\n",
    "# # Install `transformers` from master\n",
    "# !pip install git+https://github.com/huggingface/transformers\n",
    "# !pip list | grep -E 'transformers|tokenizers'\n",
    "# !pip install nlp==0.2.0\n",
    "# !pip install datasets\n",
    "# !pip install git+https://github.com/huggingface/nlp\n",
    "# \n",
    "# # transformers version at notebook update --- 2.11.0\n",
    "# # tokenizers version at notebook update --- 0.8.0rc1"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fetch datasets"
   ],
   "metadata": {
    "id": "b4Jowf-vf8Ck"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import tokenize\n",
    "import dis\n",
    "import sys\n",
    "import re\n",
    "import keyword\n",
    "import pandas as pd\n",
    "import ast\n",
    "import torch\n",
    "import signal\n",
    "from functools import wraps\n",
    "\n",
    "def multireplace(string, replacements, ignore_case=False):\n",
    "    \"\"\"\n",
    "    Given a string and a replacement map, it returns the replaced string.\n",
    "    :param str string: string to execute replacements on\n",
    "    :param dict replacements: replacement dictionary {value to find: value to replace}\n",
    "    :param bool ignore_case: whether the match should be case insensitive\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    # If case insensitive, we need to normalize the old string so that later a replacement\n",
    "    # can be found. For instance with {\"HEY\": \"lol\"} we should match and find a replacement for \"hey\",\n",
    "    # \"HEY\", \"hEy\", etc.\n",
    "    if ignore_case:\n",
    "        def normalize_old(s):\n",
    "            return s.lower()\n",
    "        re_mode = re.IGNORECASE\n",
    "    else:\n",
    "        def normalize_old(s):\n",
    "            return s\n",
    "        re_mode = 0\n",
    "\n",
    "    replacements = {normalize_old(key): val for key, val in replacements.items()}\n",
    "    \n",
    "    # Place longer ones first to keep shorter substrings from matching where the longer ones should take place\n",
    "    # For instance given the replacements {'ab': 'AB', 'abc': 'ABC'} against the string 'hey abc', it should produce\n",
    "    # 'hey ABC' and not 'hey ABc'\n",
    "    rep_sorted = sorted(replacements, key=len, reverse=True)\n",
    "    rep_escaped = map(re.escape, rep_sorted)\n",
    "    \n",
    "    # Create a big OR regex that matches any of the substrings to replace\n",
    "    pattern = re.compile(\"|\".join(rep_escaped), re_mode)\n",
    "    \n",
    "    # For each match, look up the new string in the replacements, being the key the normalized old string\n",
    "    return pattern.sub(lambda match: replacements[normalize_old(match.group(0))], string)\n",
    "\n",
    "\n",
    "def convert(file, output_file):\n",
    "    with open (file, \"r\") as f:\n",
    "        text = f.read()  \n",
    "\n",
    "    replacements = {}\n",
    "    for node in ast.iter_child_nodes(ast.parse(text)):\n",
    "        if isinstance(node, ast.ImportFrom):\n",
    "            replacements.update({node.module: 'MODULE'})\n",
    "        if isinstance(node, ast.Import) or isinstance(node, ast.ImportFrom):\n",
    "            for i, v in enumerate(node.names):\n",
    "                if(node.names[i].asname):\n",
    "                    replacements.update({node.names[i].name: 'LIB'})                \n",
    "                    replacements.update({node.names[i].asname: 'ALIAS'})\n",
    "                else:\n",
    "                    replacements.update({node.names[i].name: 'LIBRARY'})\n",
    "\n",
    "\n",
    "    # reomve * from the dictionary (handle from module import * statement)\n",
    "    replacements.pop('*', None)\n",
    "    print('List of modules and libraries to replace:\\n', replacements)\n",
    "\n",
    "    with open('med.py','w') as f:\n",
    "        f.write(multireplace(text, replacements, ignore_case = True))\n",
    "\n",
    "    file = 'med.py'\n",
    "    with open(file,'rb') as f:\n",
    "        tokens = list(tokenize.tokenize(f.readline))\n",
    "        \n",
    "    ### extract important data from the output of tokenize package\n",
    "    toks = pd.DataFrame(columns = ['original','type','text', 'line','pos'])\n",
    "\n",
    "    last_line = 0\n",
    "    last_pos = 0\n",
    "\n",
    "    for token in tokens:\n",
    "        \n",
    "        tok_org = token.string\n",
    "        tok_text = token.string    \n",
    "        tok_type = str(token).split('(')[2].split(')')[0]\n",
    "\n",
    "        # convert keywords to upper\n",
    "        if keyword.iskeyword(tok_text):\n",
    "            tok_type = str.upper(tok_text)\n",
    "        \n",
    "        #extract operations\n",
    "        # if tok_type == 'OP':\n",
    "        #     tok_type = tok_text\n",
    "\n",
    "\n",
    "        # getting rid of comments and empty lines\n",
    "        if tok_type in ['NL','NEWLINE','COMMENT']:\n",
    "            continue\n",
    "        \n",
    "        #retrieve the position\n",
    "        tok_line = token.start[0]\n",
    "        \n",
    "        if last_line == tok_line:\n",
    "            last_pos +=  1\n",
    "        else:\n",
    "            last_pos = 1\n",
    "        tok_pos = last_pos\n",
    "        last_line = tok_line\n",
    "        \n",
    "        new_row = pd.DataFrame([{'type':tok_type,\n",
    "                         'original':tok_org,\n",
    "                         'text':tok_text,\n",
    "                         'line':tok_line,\n",
    "                         'pos':tok_pos}])\n",
    "        \n",
    "        toks = pd.concat([toks, new_row], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "    # remove encoding lines and end of file\n",
    "    toks.line = toks.line.astype('int')\n",
    "    toks.pos = toks.pos.astype('int')\n",
    "    toks = toks.loc[~((toks.type == 'ENCODING') | (toks.type == 'ENDMARKER'))]\n",
    "    toks['doc'] = (toks.text.str.contains('\"\"\"') | toks.text.str.contains(\"'''\"))\n",
    "    toks = toks.loc[~(toks.doc)].drop(['doc'],axis=1)\n",
    "\n",
    "    toks.head(20)\n",
    "\n",
    "    indent = 0\n",
    "    last_line = 0\n",
    "\n",
    "    for index,row in toks.iterrows():\n",
    "        if row.type == \"INDENT\":\n",
    "            indent +=1\n",
    "            continue\n",
    "        if row.type == \"DEDENT\":\n",
    "            indent -=1\n",
    "            continue\n",
    "        if row.line != last_line:\n",
    "            last_line = row.line            \n",
    "            new_row = pd.DataFrame([{'type':'\\n'+indent*'\\t',\n",
    "                                'text':'\\n'+indent*'\\t',\n",
    "                                'line':row.line,\n",
    "                                'pos':row.pos-1}])\n",
    "            toks = pd.concat([toks, new_row], ignore_index=True)\n",
    "\n",
    "\n",
    "    toks = toks.loc[~((toks.type=='INDENT') | (toks.type=='DEDENT'))]\n",
    "    toks = toks.sort_values(['line','pos']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    # drop the first row (empty line)\n",
    "    toks.drop(toks.index[:1], inplace=True)\n",
    "\n",
    "    toks.head(20)\n",
    "\n",
    "    with open(file,'r') as f:\n",
    "        src = f.read()\n",
    "\n",
    "    stdout_backup = sys.stdout\n",
    "    sys.stdout = open('dis.txt','w')\n",
    "    dis.dis(src)\n",
    "    sys.stdout = stdout_backup\n",
    "\n",
    "    with open('dis.txt','r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # find global variables\n",
    "    glbls = [].copy()    \n",
    "    for l in lines:\n",
    "        clean = l.replace('>>',' ').strip().split()\n",
    "        if len(clean):\n",
    "            try:\n",
    "                int(clean[1])\n",
    "                line = int(clean[0])\n",
    "            except:\n",
    "                clean = [str(line)]+clean\n",
    "            if 'LOAD_GLOBAL' in clean:\n",
    "                print('found a global!')\n",
    "                glbls.append((int(clean[0]),clean[-1].replace('(','').replace(')','')))\n",
    "\n",
    "    for l,n in glbls:\n",
    "        toks.loc[(toks.line==l) & (toks.text==n),'type'] = 'GLOBAL_VARIABLE'\n",
    "\n",
    "    toks .head(10) \n",
    "\n",
    "    text_imports = ' '.join(list(toks.text)).replace('\\n ','\\n').replace(' \\n','\\n').replace('\\t ','\\t').replace(' . ','.').replace(' (','(')\n",
    "    text_imports = multireplace(text_imports, replacements, ignore_case = True)\n",
    "\n",
    "    with open('normalized_textual_file.py','w') as f:\n",
    "        f.write(text_imports)\n",
    "\n",
    "    toks.type = toks.apply(lambda x: x['text'] if str(x['text']) in ['LIBRARY','LIB','ALIAS','MODULE'] else x['type'], axis = 1)\n",
    "    code_converted = ' '.join(list(toks.type)).replace('\\n ','\\n').replace(' \\n','\\n').replace('\\t ','\\t').replace(' . ','.').replace(' (','(')\n",
    "\n",
    "    final_replacements = {'GLOBAL_VARIABLE(':'FUNCTION_CALL(',                      \n",
    "    #                       'NAME.NAME':'NAME',\n",
    "                          'NAME(':'FUNCTION_CALL(',\n",
    "                          'NAME':'LOCAL_VARIABLE'}\n",
    "\n",
    "    code_converted = multireplace(code_converted, final_replacements, ignore_case = False)\n",
    "\n",
    "    with open(output_file,'w') as f:\n",
    "        f.write(code_converted)\n",
    "\n",
    "\n",
    "WEIGHT_MATRIX = {\n",
    "        'NUMBER' : [1.625, 1.25, 1.125],\n",
    "        'NAME' : [1.625, 1.125, 1.5],\n",
    "        'LOCAL_VARIABLE' : [1.625, 1.125, 1.5],\n",
    "        'FUNCTION_NAME' : [1.625, 1.25, 1.5]\n",
    "    }\n",
    "\n",
    "\n",
    "input_file = \"/tmp/input_file.txt\"\n",
    "output_file = \"/tmp/output_file.txt\"\n",
    "\n",
    "\n",
    "def reranking_layer(outputs, context, tokenizer):\n",
    "  with open(input_file, 'w') as f:\n",
    "    f.write(context);\n",
    "  \n",
    "  convert(file_path=input_file, output_file=output_file)\n",
    "  with open(output_file, 'rb') as context:\n",
    "    inputs = list(zip(tokenizer(input_file), tokenizer(output_file)))\n",
    "    for item in inputs:\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor(WEIGHT_MATRIX[item[1]]))\n"
   ],
   "metadata": {
    "id": "PGHNJPreFmOw",
    "ExecuteTime": {
     "end_time": "2024-12-19T10:28:28.632652Z",
     "start_time": "2024-12-19T10:28:28.601861Z"
    }
   },
   "execution_count": 60,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "convert(\"../dataset/sample_data/data/peakfinder.py\", \n",
    "        \"../dataset/sample_data/data/converted_train.txt\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cNqN20xrhkGq",
    "outputId": "7e041a8c-eb0a-4648-9de0-3c1ffc6440e6",
    "ExecuteTime": {
     "end_time": "2024-12-19T10:28:29.205795Z",
     "start_time": "2024-12-19T10:28:28.634651Z"
    }
   },
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of modules and libraries to replace:\n",
      " {'warnings': 'LIBRARY', 'numpy': 'LIB', 'np': 'ALIAS', 'astropy.table': 'MODULE', 'QTable': 'LIBRARY', 'scipy.ndimage': 'MODULE', 'maximum_filter': 'LIBRARY', 'photutils.utils._misc': 'MODULE', '_get_meta': 'LIBRARY', 'photutils.utils._parameters': 'MODULE', 'as_pair': 'LIBRARY', 'photutils.utils._quantity_helpers': 'MODULE', 'process_quantities': 'LIBRARY', 'photutils.utils._stats': 'MODULE', 'nanmin': 'LIBRARY', 'photutils.utils.exceptions': 'MODULE', 'NoDetectionsWarning': 'LIBRARY'}\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# pretrain dataset\n",
    "#!wget https://huggingface.co/rgismondi/python-50k-dedup/blob/main/pretrain_dataset.zip\n",
    "#!unzip 'pretrain_dataset.zip'\n",
    "\n",
    "# converted dataset\n",
    "#! wget https://huggingface.co/rgismondi/python-50k-dedup/blob/main/converted_dataset.zip\n",
    "#! unzip 'converted_dataset.zip'\n",
    "\n",
    "# test dataset\n",
    "#!wget https://huggingface.co/rgismondi/python-50k-dedup/blob/main/finetune_eval_dataset.zip\n",
    "#!unzip 'finetune_eval_dataset.zip'"
   ],
   "metadata": {
    "id": "sEmqUukXf__k",
    "ExecuteTime": {
     "end_time": "2024-12-19T10:28:29.220794Z",
     "start_time": "2024-12-19T10:28:29.207788Z"
    }
   },
   "execution_count": 62,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train a customised python byte-level Byte-pair encoding tokenizer. "
   ],
   "metadata": {
    "id": "M0wmpgCxUIF3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer,TextDataset,DataCollatorForLanguageModeling\n",
    "import glob\n",
    "import random \n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../local_gpt2_tokenizer/\", local_files_only=True)"
   ],
   "metadata": {
    "id": "oPq1Bau8UbpB",
    "ExecuteTime": {
     "end_time": "2024-12-19T10:28:29.346336Z",
     "start_time": "2024-12-19T10:28:29.221796Z"
    }
   },
   "execution_count": 63,
   "outputs": []
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../dataset/sample_data/converted/peakfinder.txt\n",
      "List of modules and libraries to replace:\n",
      " {'warnings': 'LIBRARY', 'numpy': 'LIB', 'np': 'ALIAS', 'astropy.table': 'MODULE', 'QTable': 'LIBRARY', 'scipy.ndimage': 'MODULE', 'maximum_filter': 'LIBRARY', 'photutils.utils._misc': 'MODULE', '_get_meta': 'LIBRARY', 'photutils.utils._parameters': 'MODULE', 'as_pair': 'LIBRARY', 'photutils.utils._quantity_helpers': 'MODULE', 'process_quantities': 'LIBRARY', 'photutils.utils._stats': 'MODULE', 'nanmin': 'LIBRARY', 'photutils.utils.exceptions': 'MODULE', 'NoDetectionsWarning': 'LIBRARY'}\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n",
      "found a global!\n",
      "Here is train.txt && test.txt!\n",
      "Here is converted_train.txt && converted_test.txt!\n"
     ]
    }
   ],
   "source": [
    "# ../dataset/sample_data/data/peakfinder.py\n",
    "\n",
    "paths = [str(x) for x in Path(\".\").glob(\"../dataset/sample_data/data/*.py\")]\n",
    "converted_paths = []\n",
    "for path in paths:\n",
    "  # converted_path = \"../dataset/sample_data/converted/\"+ path.split(\"/\").pop().split(\".\")[0] + \".txt\"\n",
    "  converted_path = \"../dataset/sample_data/converted/\"+ Path(path).stem + \".txt\"\n",
    "  print(converted_path)\n",
    "  try:\n",
    "    convert(path, converted_path)\n",
    "    converted_paths.append(converted_path)\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "with open(\"./train.txt\", \"wb\") as train_outfile:\n",
    "  with open(\"./test.txt\", \"wb\") as test_outfile:\n",
    "    print(\"Here is train.txt && test.txt!\")\n",
    "    for f in paths:\n",
    "        choice = random.random()\n",
    "        with open(f, \"rb\") as infile:\n",
    "            if choice > 0.1:\n",
    "              train_outfile.write(infile.read())\n",
    "            else:\n",
    "              test_outfile.write(infile.read())\n",
    "\n",
    "with open(\"./converted_train.txt\", \"wb\") as train_outfile:\n",
    "  with open(\"./converted_test.txt\", \"wb\") as test_outfile:\n",
    "    print(\"Here is converted_train.txt && converted_test.txt!\")\n",
    "    for f in converted_paths:\n",
    "        choice = random.random()\n",
    "        with open(f, \"rb\") as infile:\n",
    "            if choice > 0.1:\n",
    "              train_outfile.write(infile.read())\n",
    "            else:\n",
    "              test_outfile.write(infile.read())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T10:28:29.987154Z",
     "start_time": "2024-12-19T10:28:29.348337Z"
    }
   },
   "execution_count": 64
  },
  {
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "def load_dataset(train_path,test_path,tokenizer):\n",
    "    train_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=train_path,\n",
    "          block_size=32) # 128->32\n",
    "     \n",
    "    test_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=test_path,\n",
    "          block_size=128)   \n",
    "    \n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False,\n",
    "    )\n",
    "    return train_dataset,test_dataset,data_collator\n",
    "\n",
    "train_dataset,test_dataset,data_collator = load_dataset(\"./train.txt\", \"./test.txt\", tokenizer)\n",
    "\n",
    "with open(\"./train.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "    print(\"First 5 lines of train.txt:\")\n",
    "    print(lines[:5])\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "\n",
    "converted_train_dataset, converted_test_dataset, converted_datacollator = load_dataset(\"./converted_train.txt\", \"./converted_test.txt\", tokenizer)\n",
    "\n",
    "#pretrain_raw_files = glob.glob(\"./pretrain_dataset\" + '/**/*.py', recursive=True)\n",
    "#pretrain_converted_files = glob.glob(\"./pretrain_converted_dataset\" + '/**/*.py', recursive=True)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Q4kLrTZXTjZ",
    "outputId": "15e23b41-9245-454f-d9d6-c169eae0f943",
    "ExecuteTime": {
     "end_time": "2024-12-19T10:28:30.002666Z",
     "start_time": "2024-12-19T10:28:29.989154Z"
    }
   },
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 lines of train.txt:\n",
      "['# Licensed under a 3-clause BSD style license - see LICENSE.rst\\n', '\"\"\"\\n', 'This module provides tools for finding local peaks in an astronomical\\n', 'image.\\n', '\"\"\"\\n']\n",
      "Train dataset size: 116\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer(\"for i in range(10)\")[\"input_ids\"]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0AraoltupXmb",
    "outputId": "c90124e7-3695-44af-827d-c355632ec2af",
    "ExecuteTime": {
     "end_time": "2024-12-19T10:28:30.034665Z",
     "start_time": "2024-12-19T10:28:30.003667Z"
    }
   },
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "[1640, 1312, 287, 2837, 7, 940, 8]"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "# import nlp\n",
    "import logging\n",
    "# from datasets import load_dataset\n",
    "from transformers import TextDataset,DataCollatorForLanguageModeling\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "dataset_dict = {\n",
    "    \"token\": train_dataset,\n",
    "    \"token_type\": train_dataset,\n",
    "    \"line\": train_dataset,\n",
    "}\n",
    "\n",
    "print(dataset_dict[\"token\"])\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ePqOauaqjnZ",
    "outputId": "b6959f0d-812a-4cf2-95e4-2718a5be58bd",
    "ExecuteTime": {
     "end_time": "2024-12-19T10:28:30.049665Z",
     "start_time": "2024-12-19T10:28:30.035665Z"
    }
   },
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.data.datasets.language_modeling.TextDataset object at 0x000002C2513E4280>\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers.utils.dummy_pt_objects import GPT2LMHeadModel\n",
    "from transformers import GPT2Tokenizer\n",
    "from transformers import GPT2Config, EncoderDecoderConfig, EncoderDecoderModel\n",
    "\n",
    "\n",
    "class MultitaskModel(transformers.PreTrainedModel):\n",
    "    def __init__(self, encoder, taskmodels_dict):\n",
    "        \"\"\"\n",
    "        Setting MultitaskModel up as a PretrainedModel allows us\n",
    "        to take better advantage of Trainer features\n",
    "        \"\"\"\n",
    "        super().__init__(transformers.PretrainedConfig())\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.taskmodels_dict = nn.ModuleDict(taskmodels_dict)\n",
    "\n",
    "    def _get_models(self):\n",
    "      return self.taskmodels_dict\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, model_name, model_type_dict, model_config_dict):\n",
    "        \"\"\"\n",
    "        This creates a MultitaskModel using the model class and config objects\n",
    "        from single-task models. \n",
    "\n",
    "        We do this by creating each single-task model, and having them share\n",
    "        the same encoder transformer.\n",
    "        \"\"\"\n",
    "        shared_encoder = None\n",
    "        taskmodels_dict = {}\n",
    "        for task_name, model_type in model_type_dict.items():\n",
    "            # model = model_type.from_pretrained( \"gpt2\",\n",
    "            #     config=model_config_dict[task_name],\n",
    "            # )\n",
    "            model = model_type.from_pretrained(\n",
    "                \"../local_gpt2_tokenizer\",\n",
    "                config=model_config_dict[task_name],\n",
    "            )\n",
    "            if shared_encoder is None:\n",
    "                shared_encoder = cls.get_encoder(model)\n",
    "            else:\n",
    "                setattr(model, \"encoder\", shared_encoder)\n",
    "            taskmodels_dict[task_name] = model\n",
    "        return cls(encoder=shared_encoder, taskmodels_dict=taskmodels_dict)\n",
    "    \n",
    "\n",
    "    @classmethod\n",
    "    def get_encoder(cls, model):\n",
    "        \"\"\"\n",
    "        The encoder transformer is named differently in each model \"architecture\".\n",
    "        This method lets us get the name of the encoder attribute\n",
    "        \"\"\"\n",
    "        model_class_name = model.__class__.__name__\n",
    "        if model_class_name.startswith(\"Roberta\"):\n",
    "            return \"roberta-base\"\n",
    "        elif model_class_name.startswith(\"GPT2\"):\n",
    "            config = EncoderDecoderConfig.from_encoder_decoder_configs(model.config, model.config) \n",
    "            encoder_decoder = EncoderDecoderModel(config=config)\n",
    "            return encoder_decoder.config.encoder\n",
    "        else:\n",
    "            raise KeyError(f\"Add support for new model {model_class_name}\")\n",
    "    \n",
    "    def forward(self, task_name, **kwargs):\n",
    "        return self.taskmodels_dict[task_name](**kwargs)"
   ],
   "metadata": {
    "id": "jJLxsM_H4A6z",
    "ExecuteTime": {
     "end_time": "2024-12-19T10:28:30.064673Z",
     "start_time": "2024-12-19T10:28:30.050666Z"
    }
   },
   "execution_count": 68,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# model_name = \"gpt2\"\n",
    "# multitask_model = MultitaskModel.create(\n",
    "#     model_name=model_name,\n",
    "#     model_type_dict={\n",
    "#         \"token\": transformers.AutoModelWithLMHead,\n",
    "#         \"token_type\": transformers.AutoModelWithLMHead,\n",
    "#         \"line\": transformers.AutoModelForSequenceClassification,\n",
    "#     },\n",
    "#     model_config_dict={\n",
    "#         \"token\": transformers.AutoConfig.from_pretrained(model_name),\n",
    "#         \"token_type\": transformers.AutoConfig.from_pretrained(model_name),\n",
    "#         \"line\": transformers.AutoConfig.from_pretrained(model_name),\n",
    "#     },\n",
    "# )"
   ],
   "metadata": {
    "id": "9bbwa7E74Q0x",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "4aa35f34-7709-464a-8c6c-bc966a7f3be9",
    "ExecuteTime": {
     "end_time": "2024-12-19T10:28:30.080670Z",
     "start_time": "2024-12-19T10:28:30.065665Z"
    }
   },
   "execution_count": 69,
   "outputs": []
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at ../local_gpt2_tokenizer and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoModelForSequenceClassification, AutoConfig\n",
    "\n",
    "# 设置本地模型路径\n",
    "local_model_path = \"../local_gpt2_tokenizer\"\n",
    "\n",
    "multitask_model = MultitaskModel.create(\n",
    "    model_name=local_model_path,  # 使用本地路径代替预训练模型名称\n",
    "    model_type_dict={\n",
    "        \"token\": AutoModelForCausalLM,  # 用于生成任务（如GPT-2）\n",
    "        \"token_type\": AutoModelForCausalLM,  # 用于token类型的生成任务\n",
    "        \"line\": AutoModelForSequenceClassification,  # 用于分类任务\n",
    "    },\n",
    "    model_config_dict={\n",
    "        \"token\": AutoConfig.from_pretrained(local_model_path),  # 从本地路径加载配置\n",
    "        \"token_type\": AutoConfig.from_pretrained(local_model_path),  # 从本地路径加载配置\n",
    "        \"line\": AutoConfig.from_pretrained(local_model_path),  # 从本地路径加载配置\n",
    "    },\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T10:28:35.076402Z",
     "start_time": "2024-12-19T10:28:30.083666Z"
    }
   },
   "execution_count": 70
  },
  {
   "cell_type": "code",
   "source": [
    "# Check that we have a GPU\n",
    "!nvidia-smi\n",
    "# Check that PyTorch sees it\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CNu7NKoYpvCP",
    "outputId": "ae4d73f1-4657-4580-b85a-bfd1410bee7b",
    "ExecuteTime": {
     "end_time": "2024-12-19T10:28:35.184402Z",
     "start_time": "2024-12-19T10:28:35.077403Z"
    }
   },
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Dec 19 18:28:35 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 551.76                 Driver Version: 551.76         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060 ...  WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   54C    P8             18W /  120W |    5893MiB /   6144MiB |      9%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      1080    C+G   ...2txyewy\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A      2780    C+G   ...siveControlPanel\\SystemSettings.exe      N/A      |\n",
      "|    0   N/A  N/A      4844      C   ...\\anaconda3\\envs\\CodeFill\\python.exe      N/A      |\n",
      "|    0   N/A  N/A     10440    C+G   ...\\PyCharm 2023.3.4\\bin\\pycharm64.exe      N/A      |\n",
      "|    0   N/A  N/A     10448    C+G   ...GeForce Experience\\NVIDIA Share.exe      N/A      |\n",
      "|    0   N/A  N/A     11336    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe      N/A      |\n",
      "|    0   N/A  N/A     12136    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe      N/A      |\n",
      "|    0   N/A  N/A     13208    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe      N/A      |\n",
      "|    0   N/A  N/A     13348    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     14080    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A     14508    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A     16560    C+G   ...crosoft Office\\Office16\\WINWORD.EXE      N/A      |\n",
      "|    0   N/A  N/A     16996    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A     17936    C+G   C:\\AppData\\Mozilla Firefox\\firefox.exe      N/A      |\n",
      "|    0   N/A  N/A     18532    C+G   ...on\\131.0.2903.99\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     18956    C+G   C:\\AppData\\Mozilla Firefox\\firefox.exe      N/A      |\n",
      "|    0   N/A  N/A     20736    C+G   ...8.4.0_x64__gqbn7fs4pywxm\\Db.App.exe      N/A      |\n",
      "|    0   N/A  N/A     20824    C+G   ...1.0_x64__8wekyb3d8bbwe\\Video.UI.exe      N/A      |\n",
      "|    0   N/A  N/A     21776    C+G   ...64__8wekyb3d8bbwe\\CalculatorApp.exe      N/A      |\n",
      "|    0   N/A  N/A     22028    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     23496    C+G   ...crosoft\\Edge\\Application\\msedge.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import dataclasses\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from transformers.data.data_collator import DataCollatorForLanguageModeling, InputDataClass, DefaultDataCollator\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "from typing import List, Union, Dict\n",
    "from transformers import Trainer\n",
    "from random import random\n",
    "\n",
    "\n",
    "class NLPDataCollator(DataCollatorForLanguageModeling):\n",
    "    \"\"\"\n",
    "    Extending the existing DataCollator to work with NLP dataset batches\n",
    "    \"\"\"\n",
    "    def collate_batch(self, features: List[Union[InputDataClass, Dict]]) -> Dict[str, torch.Tensor]:\n",
    "        first = features[0]\n",
    "        if isinstance(first, dict):\n",
    "          # NLP data sets current works presents features as lists of dictionary\n",
    "          # (one per example), so we  will adapt the collate_batch logic for that\n",
    "          if \"labels\" in first and first[\"labels\"] is not None:\n",
    "              if first[\"labels\"].dtype == torch.int64:\n",
    "                  labels = torch.tensor([f[\"labels\"] for f in features], dtype=torch.long)\n",
    "              else:\n",
    "                  labels = torch.tensor([f[\"labels\"] for f in features], dtype=torch.float)\n",
    "              batch = {\"labels\": labels}\n",
    "          for k, v in first.items():\n",
    "              if k != \"labels\" and v is not None and not isinstance(v, str):\n",
    "                  batch[k] = torch.stack([f[k] for f in features])\n",
    "          return batch\n",
    "        else:\n",
    "          # otherwise, revert to using the default collate_batch\n",
    "          return DefaultDataCollator().collate_batch(features)\n",
    "\n",
    "\n",
    "class StrIgnoreDevice(str):\n",
    "    \"\"\"\n",
    "    This is a hack. The Trainer is going call .to(device) on every input\n",
    "    value, but we need to pass in an additional `task_name` string.\n",
    "    This prevents it from throwing an error\n",
    "    \"\"\"\n",
    "    def to(self, device):\n",
    "        return self\n",
    "\n",
    "class DataLoaderWithTaskname:\n",
    "    \"\"\"\n",
    "    Wrapper around a DataLoader to also yield a task name\n",
    "    \"\"\"\n",
    "    def __init__(self, task_name, data_loader):\n",
    "        self.task_name = task_name\n",
    "        self.data_loader = data_loader\n",
    "\n",
    "        self.batch_size = data_loader.batch_size\n",
    "        self.dataset = data_loader.dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_loader)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for batch in self.data_loader:\n",
    "            batch[\"task_name\"] = StrIgnoreDevice(self.task_name)\n",
    "            yield batch\n",
    "\n",
    "\n",
    "class MultitaskDataloader:\n",
    "    \"\"\"\n",
    "    Data loader that combines and samples from multiple single-task\n",
    "    data loaders.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataloader_dict):\n",
    "        self.dataloader_dict = dataloader_dict\n",
    "        self.num_batches_dict = {\n",
    "            task_name: len(dataloader) \n",
    "            for task_name, dataloader in self.dataloader_dict.items()\n",
    "        }\n",
    "        self.task_name_list = list(self.dataloader_dict)\n",
    "        self.dataset = [None] * sum(\n",
    "            len(dataloader.dataset) \n",
    "            for dataloader in self.dataloader_dict.values()\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(self.num_batches_dict.values())\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        For each batch, sample a task, and yield a batch from the respective\n",
    "        task Dataloader.\n",
    "\n",
    "        We use size-proportional sampling, but you could easily modify this\n",
    "        to sample from some-other distribution.\n",
    "        \"\"\"\n",
    "        task_choice_list = []\n",
    "        for i, task_name in enumerate(self.task_name_list):\n",
    "            task_choice_list += [i] * self.num_batches_dict[task_name]\n",
    "        task_choice_list = np.array(task_choice_list)\n",
    "        np.random.shuffle(task_choice_list)\n",
    "        dataloader_iter_dict = {\n",
    "            task_name: iter(dataloader) \n",
    "            for task_name, dataloader in self.dataloader_dict.items()\n",
    "        }\n",
    "        for task_choice in task_choice_list:\n",
    "            task_name = self.task_name_list[task_choice]\n",
    "            yield next(dataloader_iter_dict[task_name]) \n",
    "\n",
    "class MultitaskTrainer(transformers.Trainer):\n",
    "\n",
    "    def get_single_train_dataloader(self, task_name, train_dataset):\n",
    "        \"\"\"\n",
    "        Create a single-task data loader that also yields task names\n",
    "        \"\"\"\n",
    "        if self.train_dataset is None:\n",
    "            raise ValueError(\"Trainer: training requires a train_dataset.\")\n",
    "        \n",
    "        train_sampler = (\n",
    "            RandomSampler(train_dataset)\n",
    "            if self.args.local_rank == -1\n",
    "            else DistributedSampler(train_dataset)\n",
    "        )\n",
    "\n",
    "        data_loader = DataLoaderWithTaskname(\n",
    "            task_name=task_name,\n",
    "            data_loader=DataLoader(\n",
    "              train_dataset,\n",
    "              batch_size=self.args.train_batch_size,\n",
    "              sampler=train_sampler\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        return data_loader\n",
    "\n",
    "    def get_train_dataloader(self):\n",
    "        \"\"\"\n",
    "        Returns a MultitaskDataloader, which is not actually a Dataloader\n",
    "        but an iterable that returns a generator that samples from each \n",
    "        task Dataloader\n",
    "        \"\"\"\n",
    "        return MultitaskDataloader({\n",
    "            task_name: self.get_single_train_dataloader(task_name, task_dataset)\n",
    "            for task_name, task_dataset in self.train_dataset.items()\n",
    "        })\n",
    "    \n",
    "    def train(self):\n",
    "      # config = transformers.AutoConfig.from_pretrained(\"gpt2\")\n",
    "      # model = transformers.AutoModelWithLMHead.from_pretrained(\"gpt2\", config=config)\n",
    "      config = transformers.AutoConfig.from_pretrained(\"../local_gpt2_tokenizer\")\n",
    "      model = transformers.AutoModelWithLMHead.from_pretrained(\"../local_gpt2_tokenizer\", config=config)\n",
    "      trainer = Trainer(\n",
    "        model=model,\n",
    "        args=transformers.TrainingArguments(\n",
    "          output_dir=\"./models/multitask_model\",\n",
    "          overwrite_output_dir=True,\n",
    "          learning_rate=1e-5,\n",
    "          do_train=True,\n",
    "          num_train_epochs=100,\n",
    "          # Adjust batch size if this doesn't fit on the Colab GPU\n",
    "          per_device_train_batch_size=8,  \n",
    "          per_device_eval_batch_size=8,\n",
    "          save_steps=3000,\n",
    "        ),\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "      )\n",
    "      trainer.train()\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=True):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        reranking_layer(outputs, inputs._get_value(), tokenizer=tokenizer) #input value is tensor\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 2.0, 3.0]))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ],
   "metadata": {
    "id": "dzVHNee8EP-T",
    "ExecuteTime": {
     "end_time": "2024-12-19T10:28:35.215403Z",
     "start_time": "2024-12-19T10:28:35.186402Z"
    }
   },
   "execution_count": 72,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "tzNxUL7gLsYV",
    "ExecuteTime": {
     "end_time": "2024-12-19T10:28:35.230404Z",
     "start_time": "2024-12-19T10:28:35.217402Z"
    }
   },
   "execution_count": 72,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "trainer = MultitaskTrainer(\n",
    "    model=multitask_model,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=\"./models/multitask_model\",\n",
    "        overwrite_output_dir=True,\n",
    "        learning_rate=1e-5,\n",
    "        do_train=True,\n",
    "        num_train_epochs=1, #100\n",
    "        # Adjust batch size if this doesn't fit on the Colab GPU\n",
    "        per_device_train_batch_size=8,  \n",
    "        # per_device_eval_batch_size=8,\n",
    "        save_steps=3000,\n",
    "    ),\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TJ3CrZfgHGKq",
    "outputId": "30aa520a-c8c4-4b53-8659-9f30b068562c",
    "ExecuteTime": {
     "end_time": "2024-12-19T10:28:36.023116Z",
     "start_time": "2024-12-19T10:28:35.231403Z"
    }
   },
   "execution_count": 73,
   "outputs": []
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T10:28:36.039108Z",
     "start_time": "2024-12-19T10:28:36.024108Z"
    }
   },
   "execution_count": 74
  },
  {
   "cell_type": "code",
   "source": [
    "# preds_dict = {}\n",
    "# for task_name in [\"token\", \"token_type\", \"line\"]:\n",
    "#   eval_dataloader = DataLoaderWithTaskname(\n",
    "#       task_name,\n",
    "#       trainer.get_eval_dataloader(eval_dataset=dataset_dict[task_name])\n",
    "#   )\n",
    "# \n",
    "#   print(f\"Eval DataLoader batch_size: {eval_dataloader.batch_size}\")\n",
    "#   print(eval_dataloader.data_loader.collate_fn)\n",
    "# \n",
    "# \n",
    "#   preds_dict[task_name] = trainer.prediction_loop(\n",
    "#       eval_dataloader, \n",
    "#       description=f\"Validation: {task_name}\",\n",
    "#   )\n",
    "# \n",
    "# \n",
    "# print(preds_dict)"
   ],
   "metadata": {
    "id": "Xgw82zyxp-_5",
    "ExecuteTime": {
     "end_time": "2024-12-19T10:30:26.529744Z",
     "start_time": "2024-12-19T10:30:26.509738Z"
    }
   },
   "execution_count": 77,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval DataLoader batch_size: None\n",
      "<transformers.trainer_utils.RemoveColumnsCollator object at 0x000002C20309A490>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Batch size cannot be None. Ensure the dataloader has a valid batch_size or total_batch_size.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_4844\\3964622012.py\u001B[0m in \u001B[0;36m<cell line: 2>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 12\u001B[1;33m   preds_dict[task_name] = trainer.prediction_loop(\n\u001B[0m\u001B[0;32m     13\u001B[0m       \u001B[0meval_dataloader\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     14\u001B[0m       \u001B[0mdescription\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34mf\"Validation: {task_name}\"\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\AppData\\anaconda3\\envs\\CodeFill\\lib\\site-packages\\transformers\\trainer.py\u001B[0m in \u001B[0;36mprediction_loop\u001B[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001B[0m\n\u001B[0;32m   4808\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   4809\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mbatch_size\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 4810\u001B[1;33m             raise ValueError(\n\u001B[0m\u001B[0;32m   4811\u001B[0m                 \u001B[1;34m\"Batch size cannot be None. Ensure the dataloader has a valid batch_size or total_batch_size.\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   4812\u001B[0m             )\n",
      "\u001B[1;31mValueError\u001B[0m: Batch size cannot be None. Ensure the dataloader has a valid batch_size or total_batch_size."
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='382' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [15/15 06:21]\n    </div>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Not all data has been set. Are you sure you passed all values?\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'EvalLoopOutput' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_4844\\3570159515.py\u001B[0m in \u001B[0;36m<cell line: 48>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     73\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     74\u001B[0m     \u001B[1;31m# 调整预测结果的形状\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 75\u001B[1;33m     \u001B[1;32mfor\u001B[0m \u001B[0mkey\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalue\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mpreds_dict\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mtask_name\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     76\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     77\u001B[0m             \u001B[0mpreds_dict\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mtask_name\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0madjust_tensor_shape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[1;36m32\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m32\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m50257\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'EvalLoopOutput' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 自定义 DataLoaderWithTaskname 的封装逻辑\n",
    "class DataLoaderWithTaskname:\n",
    "    def __init__(self, task_name, data_loader):\n",
    "        \"\"\"\n",
    "        包装一个 DataLoader，同时为每个批次添加任务名称\n",
    "        \"\"\"\n",
    "        self.task_name = task_name\n",
    "        self.data_loader = data_loader\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_loader)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in self.data_loader:\n",
    "            if not isinstance(batch, dict):\n",
    "                raise ValueError(f\"Batch must be a dictionary, but got {type(batch)}.\")\n",
    "            # 直接添加字符串而非张量，以避免哈希冲突\n",
    "            batch[\"task_name\"] = self.task_name\n",
    "            yield batch\n",
    "\n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        \"\"\"\n",
    "        从内部的 data_loader 中获取 batch_size\n",
    "        \"\"\"\n",
    "        return self.data_loader.batch_size\n",
    "\n",
    "def adjust_tensor_shape(tensor, target_shape):\n",
    "    \"\"\"\n",
    "    调整 tensor 的形状以匹配目标形状，必要时进行截断或填充\n",
    "    \"\"\"\n",
    "    current_shape = tensor.shape\n",
    "    if current_shape == target_shape:\n",
    "        return tensor\n",
    "    elif len(current_shape) == len(target_shape):\n",
    "        slices = tuple(slice(0, min(c, t)) for c, t in zip(current_shape, target_shape))\n",
    "        padded = torch.zeros(target_shape, dtype=tensor.dtype, device=tensor.device)\n",
    "        padded[:current_shape[0], :current_shape[1], :current_shape[2]] = tensor[slices]\n",
    "        return padded\n",
    "    else:\n",
    "        raise ValueError(f\"Cannot adjust tensor from shape {current_shape} to {target_shape}\")\n",
    "\n",
    "preds_dict = {}\n",
    "\n",
    "# 遍历任务列表，生成评估 DataLoader 并预测\n",
    "for task_name in [\"token\", \"token_type\", \"line\"]:\n",
    "    # 获取 Trainer 的 DataLoader\n",
    "    trainer_dataloader = trainer.get_eval_dataloader(eval_dataset=dataset_dict[task_name])\n",
    "\n",
    "    # 手动检查并设置 batch_size（如果为 None）\n",
    "    if trainer_dataloader.batch_size is None:\n",
    "        trainer_dataloader = DataLoader(\n",
    "            dataset=trainer_dataloader.dataset,\n",
    "            batch_size=1,  # 修改为 batch_size=1，避免因无 pad_token 引发错误\n",
    "            shuffle=False,\n",
    "            collate_fn=trainer_dataloader.collate_fn  # 保留原来的 collate_fn\n",
    "        )\n",
    "\n",
    "    # 使用自定义包装类 DataLoaderWithTaskname\n",
    "    eval_dataloader = DataLoaderWithTaskname(task_name, trainer_dataloader)\n",
    "\n",
    "    # 打印数据加载器信息（用于调试）\n",
    "    print(f\"Eval DataLoader for task '{task_name}' created with batch_size={eval_dataloader.batch_size}\")\n",
    "    print(f\"Collate function: {eval_dataloader.data_loader.collate_fn}\")\n",
    "\n",
    "    # 使用封装后的 DataLoader 进行预测\n",
    "    preds_dict[task_name] = trainer.prediction_loop(\n",
    "        eval_dataloader,\n",
    "        description=f\"Validation: {task_name}\",\n",
    "    )\n",
    "\n",
    "    # 调整预测结果的形状\n",
    "    for key, value in preds_dict[task_name].items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            preds_dict[task_name][key] = adjust_tensor_shape(value, (32, 32, 50257))\n",
    "\n",
    "# 打印预测结果\n",
    "print(preds_dict)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-19T10:41:18.694870Z",
     "start_time": "2024-12-19T10:40:42.911628Z"
    }
   },
   "execution_count": 83
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score, label_ranking_average_precision_score\n",
    "\n",
    "accuracy_dict = {}\n",
    "mrr_dict = {}\n",
    "\n",
    "for task_name in [\"token\", \"token_type\", \"line\"]:\n",
    "  accuracy_dict[task_name] = accuracy_score(preds_dict[task_name].predictions.flatten(),\n",
    "    preds_dict[task_name].label_ids)\n",
    "  \n",
    "  mrr_dict[task_name] = label_ranking_average_precision_score(preds_dict[task_name].predictions.flatten(),\n",
    "    preds_dict[task_name].label_ids)\n",
    "  "
   ],
   "metadata": {
    "id": "XWUxUWUVE5Dq"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
